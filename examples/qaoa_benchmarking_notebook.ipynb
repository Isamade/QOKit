{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Benchmarking of Parallel QAOA Portfolio Optimization with DWE\n",
    "\n",
    "This notebook is dedicated to systematically benchmarking the performance and solution quality of our parallel QAOA implementation for portfolio optimization, leveraging the Domain Wall Encoding (DWE) technique. As a telecom engineer with a mission to raise quantum technology awareness, this empirical data will be invaluable.\n",
    "\n",
    "We will evaluate how varying problem size, QAOA depth, optimizer effort, and the number of parallel processes impact both the time to solution and the quality of the results.\n",
    "\n",
    "**Key enhancements in this benchmarking notebook:**\n",
    "1.  **Systematic Parameter Variation:** We'll iterate through predefined ranges of `N`, `p`, `max_evals`, and `num_parallel_runs`.\n",
    "2.  **Robust Data Collection:** Results from each run (including repetitions for statistical robustness) will be collected into a Pandas DataFrame.\n",
    "3.  **Comprehensive Metrics:** We'll track wall-clock time, solution energy, approximation ratio (where applicable), and the critical constraint satisfaction rate.\n",
    "4.  **Automated Visualization:** Plots will be generated to illustrate the trends and insights from the benchmarking data.\n",
    "\n",
    "**Important Note on Multiprocessing and External Files:**\n",
    "As we've discussed, the core functions for optimization (`minimize_nlopt`) and single QAOA runs (`run_single_optimization`) are now located in `qaoa_utils.py`. This best practice resolves common `AttributeError` issues in Jupyter when using `multiprocessing`, ensuring child processes can correctly access the necessary code."
   ],
   "id": "b11c490fa9d68e18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We begin by importing all required libraries. This includes standard scientific computing libraries (`numpy`, `pandas`), multiprocessing utilities (`os`, `multiprocessing`, `concurrent.futures`, `functools`), and the quantum computing specific tools (`qiskit_aer`, `qokit`).\n",
    "\n",
    "Crucially, `minimize_nlopt` and `run_single_optimization` are imported from `qaoa_utils.py`, which must be in the same directory as this notebook."
   ],
   "id": "2e2e5e7a4368a718"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import time \n",
    "import nlopt \n",
    "import multiprocessing \n",
    "from concurrent.futures import ProcessPoolExecutor \n",
    "from functools import partial \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the core functions from the external file\n",
    "from qaoa_utils import minimize_nlopt, run_single_optimization\n",
    "\n",
    "from qiskit_aer import AerSimulator\n",
    "from qokit.portfolio_optimization import get_sk_ini, portfolio_brute_force\n",
    "from qokit.qaoa_objective_portfolio import get_qaoa_portfolio_objective\n",
    "\n",
    "# Optional: for later analysis of optimal bitstring\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.primitives import Sampler\n",
    "from qokit.qaoa_circuit_portfolio import get_parameterized_qaoa_circuit"
   ],
   "id": "70bad43200af3a57",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition Function (with DWE)\n",
    "\n",
    "To enable systematic benchmarking across various problem sizes (`N`), we've encapsulated the DWE-transformed QUBO problem definition into a function. This function will generate unique problem instances (based on a `problem_seed`) for each `N` being tested, ensuring consistency and reproducibility across runs for the same `N`.\n",
    "\n",
    "For smaller `N` (e.g., up to 20), this function will also attempt to calculate the **classical brute-force optimal energy**. This allows us to compute the **Approximation Ratio (AR)**, a key metric for solution quality. For `N` larger than the brute-force feasibility limit, AR will not be calculated."
   ],
   "id": "957982d2d6175519"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def define_po_problem(N, K, q, lambda_sum, problem_seed=None):\n",
    "    \"\"\"\n",
    "    Defines the Portfolio Optimization problem with Domain Wall Encoding.\n",
    "    Generates random mu and Sigma for the given N.\n",
    "    Calculates classical brute-force solution for small N.\n",
    "    \"\"\"\n",
    "    if problem_seed is not None:\n",
    "        np.random.seed(problem_seed) # for reproducibility of problem instance definition\n",
    "\n",
    "    mu = np.random.uniform(0.05, 0.20, N)\n",
    "    Sigma = np.random.uniform(0.001, 0.015, (N, N))\n",
    "    Sigma = (Sigma + Sigma.T) / 2 \n",
    "    Sigma = Sigma + np.eye(N) * 0.005 # Ensure positive semi-definite\n",
    "\n",
    "    # --- DWE Transformation Coefficients ---\n",
    "    factor_J_obj = (2 * q) / (K**2) \n",
    "    factor_h_linear_obj = -1 / K\n",
    "    factor_h_diagonal_obj = q / (K**2) \n",
    "\n",
    "    J_coeffs_objective = {}\n",
    "    h_coeffs_objective = {}\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            J_coeffs_objective[(i, j)] = factor_J_obj * Sigma[i, j]\n",
    "\n",
    "    for i in range(N):\n",
    "        h_coeffs_objective[i] = factor_h_linear_obj * mu[i] + factor_h_diagonal_obj * Sigma[i, i]\n",
    "\n",
    "    J_coeffs_total = {}\n",
    "    h_coeffs_total = {}\n",
    "\n",
    "    for (i, j), val in J_coeffs_objective.items():\n",
    "        J_coeffs_total[(i, j)] = val + 2 * lambda_sum\n",
    "\n",
    "    for i, val in h_coeffs_objective.items():\n",
    "        h_coeffs_total[i] = val - 5 * lambda_sum\n",
    "\n",
    "    po_problem = {\n",
    "        \"N\": N,\n",
    "        \"K\": K,\n",
    "        \"q\": q, \n",
    "        \"J\": J_coeffs_total, \n",
    "        \"h\": h_coeffs_total, \n",
    "        \"means\": mu,         \n",
    "        \"cov\": Sigma,        \n",
    "        \"q_orig\": q,         \n",
    "        \"scale\": 1.0         \n",
    "    }\n",
    "\n",
    "    best_portfolio = (None, None) \n",
    "    if N <= 20: # Threshold for classical brute-force feasibility\n",
    "        try:\n",
    "            # Brute-force solver expects the original problem definition without DWE penalty\n",
    "            original_po_problem_for_brute_force = {\"N\":N, \"K\":K, \"q\":q, \"means\":mu, \"cov\":Sigma}\n",
    "            best_portfolio = portfolio_brute_force(original_po_problem_for_brute_force, return_bitstring=False)\n",
    "            # print(f\"  Brute-force classical optimal energy for N={N}: {best_portfolio[0]:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Brute-force calculation failed for N={N}: {e}. Approximation Ratio will be 'N/A'.\")\n",
    "    \n",
    "    return po_problem, best_portfolio\n",
    "\n",
    "# Define global constant problem parameters that won't be varied in loops for this benchmark\n",
    "GLOBAL_Q = 0.5 # Risk aversion parameter\n",
    "GLOBAL_LAMBDA_SUM = 100 # DWE-inspired penalty coefficient for sum constraint"
   ],
   "id": "de12309b00f1424f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Helper Functions (`minimize_nlopt` & `run_single_optimization`)\n",
    "\n",
    "As discussed, these essential functions are crucial for the parallel execution and are now imported from `qaoa_utils.py`.\n",
    "\n",
    "* **`minimize_nlopt(f, x0, rhobeg=None, p=None, max_evals=None)`**: This function handles the classical optimization of QAOA parameters using NLopt's BOBYQA algorithm. It takes the QAOA objective function and an initial parameter guess. `max_evals` is used to limit the optimization effort.\n",
    "* **`run_single_optimization(run_config, po_problem_arg, best_portfolio_arg)`**: This function orchestrates a single QAOA optimization run. It initializes a dedicated `AerSimulator`, sets up the QAOA objective for the given problem, calls `minimize_nlopt`, and computes the final energy and approximation ratio. It's designed to be executed by each parallel process."
   ],
   "id": "d0bde2e9762a0dec"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T19:08:38.768204Z",
     "start_time": "2025-06-04T19:08:38.765823Z"
    }
   },
   "source": [
    "# The functions minimize_nlopt and run_single_optimization are imported from qaoa_utils.py.\n",
    "# Their definitions are NOT included here to prevent multiprocessing issues.\n",
    "print(\"Ensure qaoa_utils.py is in the same directory and contains `minimize_nlopt` and `run_single_optimization` functions.\")"
   ],
   "id": "861f250faaf49cd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure qaoa_utils.py is in the same directory and contains `minimize_nlopt` and `run_single_optimization` functions.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmarking Implementation\n",
    "\n",
    "This is the core of the benchmarking. We'll set up nested loops to iterate through various combinations of problem sizes (`N`), QAOA depths (`p`), `max_evals` for the classical optimizer, and the number of parallel processes. For each unique combination, we'll run the experiment multiple times (`num_repetitions_per_setting`) to gather statistically robust data.\n",
    "\n",
    "The `multiprocessing.ProcessPoolExecutor` will distribute the individual `run_single_optimization` calls across available CPU cores. `functools.partial` is used to efficiently pass the constant problem definition arguments to the parallel function."
   ],
   "id": "e7c246beb498fffa"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "all_benchmark_results = [] # List to store results from all experiments\n",
    "\n",
    "# --- Benchmarking Parameters ---\n",
    "# IMPORTANT: Adjust these ranges based on your system's capabilities and desired depth of analysis.\n",
    "# Larger values will significantly increase execution time.\n",
    "N_values = [21] # Number of assets to test\n",
    "K_ratio = 0.4 # K = int(N * K_ratio). Ratio of assets to select.\n",
    "p_values = [8] # QAOA depth (number of layers)\n",
    "max_evals_values = [1,5,10] # Max function evaluations for NLopt per QAOA run\n",
    "num_parallel_runs_values = [1, 2,4] # Number of parallel processes to use.\n",
    "                                                 # `1` serves as a baseline for speedup calculation.\n",
    "                                                 # `os.cpu_count()` uses all available logical cores.\n",
    "num_repetitions_per_setting = 1\n",
    "# How many times to repeat each (N, p, max_evals, num_procs) combination.\n",
    "                                # Essential for statistical robustness due to random initializations.\n",
    "\n",
    "# --- Seeds for Reproducibility ---\n",
    "# These offsets ensure unique seeds for problem generation and parallel runs\n",
    "PROBLEM_SEED_OFFSET = 1000 \n",
    "RUN_SEED_OFFSET = 2000 \n",
    "\n",
    "print(\"Starting comprehensive benchmarking experiments...\")\n",
    "print(f\"Total configurations to test: {len(N_values) * len(p_values) * len(max_evals_values) * len(num_parallel_runs_values) * num_repetitions_per_setting}\")\n",
    "print(\"This may take a significant amount of time based on chosen parameters.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "        print(\"Multiprocessing start method set to 'spawn'.\")\n",
    "    except RuntimeError:\n",
    "        print(\"Multiprocessing start method already set (can happen in interactive environments).\")\n",
    "\n",
    "    for N in N_values:\n",
    "        K = int(N * K_ratio)\n",
    "        print(f\"\\n----- Running for N={N} (K={K}) -----\")\n",
    "        \n",
    "        for p_val in p_values:\n",
    "            print(f\"  --- QAOA Depth (p)={p_val} ---\")\n",
    "\n",
    "            for max_evals_per_run in max_evals_values:\n",
    "                print(f\"    --- Max Evaluations per Run={max_evals_per_run} ---\")\n",
    "\n",
    "                for num_procs in num_parallel_runs_values:\n",
    "                    print(f\"      --- Parallel Processes={num_procs} ---\")\n",
    "                    \n",
    "                    for rep_idx in range(num_repetitions_per_setting):\n",
    "                        print(f\"        Repetition {rep_idx + 1}/{num_repetitions_per_setting}...\")\n",
    "                        \n",
    "                        # Regenerate the problem instance for EACH repetition to ensure different mu/Sigma.\n",
    "                        # This makes the results more general across various problem instances.\n",
    "                        # Adjust the seed generation if you want the exact same problem for all repetitions.\n",
    "                        current_problem_seed = PROBLEM_SEED_OFFSET + N + p_val + max_evals_per_run + num_procs + rep_idx\n",
    "                        po_problem, best_portfolio = define_po_problem(N, K, GLOBAL_Q, GLOBAL_LAMBDA_SUM, problem_seed=current_problem_seed)\n",
    "                        \n",
    "                        # Generate unique seeds for each parallel run within this repetition\n",
    "                        current_parallel_run_configs = [\n",
    "                            {'seed': RUN_SEED_OFFSET + (rep_idx * num_procs) + i, 'p': p_val, 'max_evals': max_evals_per_run}\n",
    "                            for i in range(num_procs) \n",
    "                        ]\n",
    "\n",
    "                        start_time = time.perf_counter()\n",
    "                        \n",
    "                        local_successful_results = []\n",
    "                        with ProcessPoolExecutor(max_workers=num_procs) as executor:\n",
    "                            # Use functools.partial to fix po_problem and best_portfolio for parallel execution\n",
    "                            func_to_map = partial(run_single_optimization, \n",
    "                                                  po_problem_arg=po_problem, \n",
    "                                                  best_portfolio_arg=best_portfolio)\n",
    "                            results_iterator = executor.map(func_to_map, current_parallel_run_configs)\n",
    "\n",
    "                            for result in results_iterator:\n",
    "                                if result is not None: \n",
    "                                    local_successful_results.append(result)\n",
    "                                \n",
    "                        end_time = time.perf_counter()\n",
    "                        total_wall_time = end_time - start_time\n",
    "\n",
    "                        # Process and store results for this specific repetition\n",
    "                        if local_successful_results:\n",
    "                            # Sort results to find the best from this set of parallel runs\n",
    "                            local_successful_results.sort(key=lambda x: x['energy'])\n",
    "                            best_run_overall = local_successful_results[0]\n",
    "                            \n",
    "                            # Re-run a quick simulation to get the actual most probable bitstring\n",
    "                            # and check its constraint satisfaction. This happens on the main process.\n",
    "                            sampler_backend = AerSimulator(method='statevector')\n",
    "                            sampler_backend.set_options(max_parallel_threads=1, statevector_parallel_threshold=N+1)\n",
    "                            \n",
    "                            gamma_opt_params = best_run_overall['optimized_params'][:p_val]\n",
    "                            beta_opt_params = best_run_overall['optimized_params'][p_val:]\n",
    "                            \n",
    "                            qaoa_circuit_for_bitstring = get_parameterized_qaoa_circuit(\n",
    "                                po_problem=po_problem,\n",
    "                                depth=p_val,\n",
    "                                ini_type='dicke',\n",
    "                                mixer_type='trotter_ring',\n",
    "                                T=1,\n",
    "                                simulator=sampler_backend, # Use the AerSimulator directly\n",
    "                                mixer_topology='linear',\n",
    "                                gamma=gamma_opt_params, \n",
    "                                beta=beta_opt_params\n",
    "                            )\n",
    "                            qaoa_circuit_for_bitstring.measure_all()\n",
    "                            \n",
    "                            sampler_job = sampler_backend.run(qaoa_circuit_for_bitstring, shots=1024)\n",
    "                            sampler_result = sampler_job.result()\n",
    "                            counts = sampler_result.get_counts(qaoa_circuit_for_bitstring)\n",
    "                            \n",
    "                            most_probable_bitstring = max(counts, key=counts.get)\n",
    "                            selected_assets_count = np.sum([int(b) for b in most_probable_bitstring])\n",
    "                            constraint_satisfied = (selected_assets_count == K)\n",
    "                            \n",
    "                            all_benchmark_results.append({\n",
    "                                'N': N,\n",
    "                                'K': K,\n",
    "                                'p': p_val,\n",
    "                                'lambda_sum': GLOBAL_LAMBDA_SUM,\n",
    "                                'max_evals_per_run': max_evals_per_run,\n",
    "                                'num_parallel_runs': num_procs,\n",
    "                                'repetition': rep_idx,\n",
    "                                'total_wall_time_s': total_wall_time,\n",
    "                                'best_energy': best_run_overall['energy'],\n",
    "                                'best_ar': best_run_overall['ar'],\n",
    "                                'most_probable_bitstring': most_probable_bitstring,\n",
    "                                'selected_assets_count': selected_assets_count,\n",
    "                                'constraint_satisfied': constraint_satisfied\n",
    "                            })\n",
    "                        else:\n",
    "                            print(f\"        Repetition {rep_idx + 1} for N={N}, p={p_val}, max_evals={max_evals_per_run}, procs={num_procs} failed to produce results.\")\n",
    "                            all_benchmark_results.append({\n",
    "                                'N': N, 'K': K, 'p': p_val, 'lambda_sum': GLOBAL_LAMBDA_SUM,\n",
    "                                'max_evals_per_run': max_evals_per_run,\n",
    "                                'num_parallel_runs': num_procs,\n",
    "                                'repetition': rep_idx,\n",
    "                                'total_wall_time_s': total_wall_time,\n",
    "                                'best_energy': np.nan, \n",
    "                                'best_ar': np.nan, \n",
    "                                'most_probable_bitstring': 'N/A',\n",
    "                                'selected_assets_count': np.nan,\n",
    "                                'constraint_satisfied': False\n",
    "                            })\n",
    "\n",
    "print(\"\\nBenchmarking experiments complete!\")\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "df_results = pd.DataFrame(all_benchmark_results)\n",
    "output_filename = \"qaoa_dwe_benchmarking_results.csv\"\n",
    "df_results.to_csv(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "display(df_results.head())"
   ],
   "id": "95637437644dad7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e65d37ab024bb9aa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 5. Analysis and Visualization\n",
    "\n",
    "Now, we'll load the collected results (or use the DataFrame directly if the previous cell just ran) and generate various plots to understand the performance trends. We'll group the data by the relevant parameters and calculate mean values to account for the variability introduced by multiple repetitions."
   ],
   "id": "beafb77b256e93b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Load the results (useful if you ran benchmarking in a separate session or restarted kernel)\n",
    "# try:\n",
    "#     df_results = pd.read_csv(\"qaoa_dwe_benchmarking_results.csv\")\n",
    "#     print(\"Loaded results from CSV.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"No CSV found. Please ensure the benchmarking section (Section 4) was run successfully to generate data.\")\n",
    "#     df_results = pd.DataFrame() # Create empty DataFrame to avoid errors if no data\n",
    "#\n",
    "# if not df_results.empty:\n",
    "#     # Calculate mean and standard deviation for key metrics across repetitions\n",
    "#     summary_df = df_results.groupby(['N', 'K', 'p', 'max_evals_per_run', 'num_parallel_runs']).agg(\n",
    "#         mean_wall_time=('total_wall_time_s', 'mean'),\n",
    "#         std_wall_time=('total_wall_time_s', 'std'),\n",
    "#         mean_energy=('best_energy', 'mean'),\n",
    "#         std_energy=('best_energy', 'std'),\n",
    "#         mean_ar=('best_ar', 'mean'),\n",
    "#         std_ar=('best_ar', 'std'),\n",
    "#         constraint_satisfaction_rate=('constraint_satisfied', lambda x: (x == True).sum() / len(x))\n",
    "#     ).reset_index()\n",
    "#\n",
    "#     print(\"\\n--- Summary of Aggregated Benchmarking Results ---\")\n",
    "#     display(summary_df.head())\n",
    "#\n",
    "#     # --- Plotting Configuration ---\n",
    "#     sns.set_theme(style=\"whitegrid\")\n",
    "#     plt.rcParams['figure.figsize'] = (10, 6)\n",
    "#     plt.rcParams['font.size'] = 12\n",
    "#\n",
    "#     # --- 1. Wall-Clock Time vs. N (Scaling Performance) ---\n",
    "#     print(\"\\n--- Plotting Wall-Clock Time vs. N ---\")\n",
    "#     # Create a plot for each 'p' value to observe its effect\n",
    "#     fig, axes = plt.subplots(1, len(p_values), figsize=(5 * len(p_values), 6), sharey=True)\n",
    "#     if len(p_values) == 1: axes = [axes] # Ensure axes is iterable even for single subplot\n",
    "#\n",
    "#     for i, p_val in enumerate(p_values):\n",
    "#         ax = axes[i]\n",
    "#         # Filter data for current 'p' and a representative 'max_evals'\n",
    "#         plot_df = summary_df[(summary_df['p'] == p_val) & (summary_df['max_evals_per_run'] == max_evals_values[0])]\n",
    "#         sns.lineplot(data=plot_df, x='N', y='mean_wall_time', hue='num_parallel_runs', marker='o', ax=ax, palette='viridis')\n",
    "#         ax.set_title(f'p={p_val}, max_evals={max_evals_values[0]}')\n",
    "#         ax.set_xlabel('Number of Assets (N)')\n",
    "#         ax.set_ylabel('Mean Wall-Clock Time (s)')\n",
    "#         ax.legend(title='Parallel Processes')\n",
    "#     plt.suptitle('Wall-Clock Time vs. N for Different Parallel Processes', fontsize=16)\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.show()\n",
    "#\n",
    "#     # --- 2. Speedup vs. Number of Parallel Runs (Parallelization Efficiency) ---\n",
    "#     print(\"\\n--- Plotting Speedup vs. Number of Parallel Runs ---\")\n",
    "#     fig, axes = plt.subplots(1, len(p_values), figsize=(5 * len(p_values), 6), sharey=True)\n",
    "#     if len(p_values) == 1: axes = [axes]\n",
    "#\n",
    "#     for i, p_val in enumerate(p_values):\n",
    "#         ax = axes[i]\n",
    "#         plot_df = summary_df[(summary_df['p'] == p_val) & (summary_df['max_evals_per_run'] == max_evals_values[0])]\n",
    "#\n",
    "#         for N_val in N_values:\n",
    "#             # Get baseline time (num_parallel_runs = 1)\n",
    "#             baseline_row = plot_df[(plot_df['N'] == N_val) & (plot_df['num_parallel_runs'] == 1)]\n",
    "#             if not baseline_row.empty and baseline_row['mean_wall_time'].iloc[0] > 0:\n",
    "#                 baseline_time = baseline_row['mean_wall_time'].iloc[0]\n",
    "#                 plot_df_N = plot_df[plot_df['N'] == N_val].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "#                 plot_df_N['speedup'] = baseline_time / plot_df_N['mean_wall_time']\n",
    "#                 sns.lineplot(data=plot_df_N, x='num_parallel_runs', y='speedup', label=f'N={N_val}', marker='o', ax=ax)\n",
    "#             else:\n",
    "#                 print(f\"Warning: Baseline for N={N_val}, p={p_val} not found or time is zero. Skipping speedup plot for this N.\")\n",
    "#\n",
    "#         # Plot ideal linear speedup\n",
    "#         ax.plot([1, max(num_parallel_runs_values)], [1, max(num_parallel_runs_values)], 'k--', label='Ideal Speedup')\n",
    "#         ax.set_title(f'p={p_val}, max_evals={max_evals_values[0]}')\n",
    "#         ax.set_xlabel('Number of Parallel Processes')\n",
    "#         ax.set_ylabel('Speedup')\n",
    "#         ax.legend(title='Problem Size (N)')\n",
    "#     plt.suptitle('Speedup vs. Number of Parallel Processes', fontsize=16)\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.show()\n",
    "#\n",
    "#     # --- 3. Mean Approximation Ratio (AR) vs. N (Solution Quality Scaling) ---\n",
    "#     print(\"\\n--- Plotting Mean Approximation Ratio (AR) vs. N ---\")\n",
    "#     # Note: AR is only available for N <= 20. The plot will show NaNs for larger N.\n",
    "#     fig, axes = plt.subplots(1, len(p_values), figsize=(5 * len(p_values), 6), sharey=False)\n",
    "#     if len(p_values) == 1: axes = [axes]\n",
    "#\n",
    "#     for i, p_val in enumerate(p_values):\n",
    "#         ax = axes[i]\n",
    "#         plot_df = summary_df[(summary_df['p'] == p_val) & (summary_df['max_evals_per_run'] == max_evals_values[0])]\n",
    "#         sns.lineplot(data=plot_df, x='N', y='mean_ar', hue='num_parallel_runs', marker='o', ax=ax, palette='viridis')\n",
    "#         ax.set_title(f'p={p_val}, max_evals={max_evals_values[0]}')\n",
    "#         ax.set_xlabel('Number of Assets (N)')\n",
    "#         ax.set_ylabel('Mean Approximation Ratio (AR)')\n",
    "#         ax.legend(title='Parallel Processes')\n",
    "#     plt.suptitle('Mean Approximation Ratio vs. N', fontsize=16)\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.show()\n",
    "#\n",
    "#     # --- 4. Mean Energy vs. max_evals (Optimization Effort vs. Quality) ---\n",
    "#     print(\"\\n--- Plotting Mean Energy vs. max_evals ---\")\n",
    "#     # Choose a fixed N and p for this plot for clarity\n",
    "#     fixed_N_for_plot = N_values[0] if N_values else None\n",
    "#     fixed_p_for_plot = p_values[0] if p_values else None\n",
    "#     fixed_procs_for_plot = num_parallel_runs_values[-1] if num_parallel_runs_values else None # Use max processes\n",
    "#\n",
    "#     if fixed_N_for_plot is not None and fixed_p_for_plot is not None and fixed_procs_for_plot is not None:\n",
    "#         plot_df = summary_df[\n",
    "#             (summary_df['N'] == fixed_N_for_plot) &\n",
    "#             (summary_df['p'] == fixed_p_for_plot) &\n",
    "#             (summary_df['num_parallel_runs'] == fixed_procs_for_plot)\n",
    "#         ]\n",
    "#         if not plot_df.empty:\n",
    "#             sns.lineplot(data=plot_df, x='max_evals_per_run', y='mean_energy', marker='o', color='red')\n",
    "#             plt.title(f'Mean Best Energy vs. max_evals (N={fixed_N_for_plot}, p={fixed_p_for_plot}, Procs={fixed_procs_for_plot})')\n",
    "#             plt.xlabel('Max Function Evaluations per Run')\n",
    "#             plt.ylabel('Mean Best Energy')\n",
    "#             plt.show()\n",
    "#         else:\n",
    "#             print(\"No data available for plotting Mean Energy vs. max_evals with selected fixed parameters.\")\n",
    "#     else:\n",
    "#         print(\"Not enough data to plot Mean Energy vs. max_evals.\")\n",
    "#\n",
    "#     # --- 5. Constraint Satisfaction Rate vs. N (DWE Effectiveness) ---\n",
    "#     print(\"\\n--- Plotting Constraint Satisfaction Rate vs. N ---\")\n",
    "#     fig, axes = plt.subplots(1, len(p_values), figsize=(5 * len(p_values), 6), sharey=True)\n",
    "#     if len(p_values) == 1: axes = [axes]\n",
    "#\n",
    "#     for i, p_val in enumerate(p_values):\n",
    "#         ax = axes[i]\n",
    "#         plot_df = summary_df[(summary_df['p'] == p_val) & (summary_df['max_evals_per_run'] == max_evals_values[0])]\n",
    "#         sns.lineplot(data=plot_df, x='N', y='constraint_satisfaction_rate', hue='num_parallel_runs', marker='o', ax=ax, palette='viridis')\n",
    "#         ax.set_title(f'p={p_val}, max_evals={max_evals_values[0]}')\n",
    "#         ax.set_xlabel('Number of Assets (N)')\n",
    "#         ax.set_ylabel('Constraint Satisfaction Rate')\n",
    "#         ax.set_ylim(-0.05, 1.05) # Ensure y-axis covers 0-1\n",
    "#         ax.legend(title='Parallel Processes')\n",
    "#     plt.suptitle('Constraint Satisfaction Rate vs. N', fontsize=16)\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.show()\n",
    "#\n",
    "# else:\n",
    "#     print(\"DataFrame is empty. Please run Section 4 to generate results before analysis.\")"
   ],
   "id": "fef69f867b948765",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os # Imported for completeness, though not strictly used in this cell for plotting\n",
    "\n",
    "# Load the results (useful if you ran benchmarking in a separate session or restarted kernel)\n",
    "try:\n",
    "    df_results = pd.read_csv(\"qaoa_dwe_benchmarking_results.csv\")\n",
    "    print(\"Loaded results from CSV.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No CSV found. Please ensure the benchmarking section (Section 4) was run successfully to generate data.\")\n",
    "    df_results = pd.DataFrame() # Create empty DataFrame to avoid errors if no data\n",
    "\n",
    "if not df_results.empty:\n",
    "    # --- FIX START ---\n",
    "    # Ensure 'best_ar' and other numeric columns are correctly typed after loading from CSV.\n",
    "    # 'errors='coerce' will turn any values that cannot be converted to numeric into NaN.\n",
    "    df_results['best_ar'] = pd.to_numeric(df_results['best_ar'], errors='coerce')\n",
    "    df_results['best_energy'] = pd.to_numeric(df_results['best_energy'], errors='coerce')\n",
    "    df_results['total_wall_time_s'] = pd.to_numeric(df_results['total_wall_time_s'], errors='coerce')\n",
    "    # --- FIX END ---\n",
    "\n",
    "    # Calculate mean and standard deviation for key metrics across repetitions\n",
    "    summary_df = df_results.groupby(['N', 'K', 'p', 'max_evals_per_run', 'num_parallel_runs']).agg(\n",
    "        mean_wall_time=('total_wall_time_s', 'mean'),\n",
    "        std_wall_time=('total_wall_time_s', 'std'),\n",
    "        mean_energy=('best_energy', 'mean'),\n",
    "        std_energy=('best_energy', 'std'),\n",
    "        mean_ar=('best_ar', 'mean'),\n",
    "        std_ar=('best_ar', 'std'),\n",
    "        constraint_satisfaction_rate=('constraint_satisfied', lambda x: (x == True).sum() / len(x))\n",
    "    ).reset_index()\n",
    "\n",
    "    print(\"\\n--- Summary of Aggregated Benchmarking Results ---\")\n",
    "    display(summary_df.head())\n",
    "\n",
    "    # --- Plotting Configuration ---\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.rcParams['figure.figsize'] = (10, 6)\n",
    "    plt.rcParams['font.size'] = 12\n",
    "\n",
    "    # Assuming N_values and p_values are defined elsewhere or need to be inferred\n",
    "    # For plotting, we need these. If not available, you might need to manually set them\n",
    "    # or extract them from the dataframe.\n",
    "    N_values = sorted(df_results['N'].unique())\n",
    "    p_values = sorted(df_results['p'].unique())\n",
    "    max_evals_values = sorted(df_results['max_evals_per_run'].unique())\n",
    "    num_parallel_runs_values = sorted(df_results['num_parallel_runs'].unique())\n",
    "\n",
    "\n",
    "    # --- 1. Wall-Clock Time vs. N (Scaling Performance) ---\n",
    "    print(\"\\n--- Plotting Wall-Clock Time vs. N ---\")\n",
    "    fig, axes = plt.subplots(1, len(p_values), figsize=(5 * len(p_values), 6), sharey=True)\n",
    "    if len(p_values) == 1: axes = [axes]\n",
    "\n",
    "    for i, p_val in enumerate(p_values):\n",
    "        ax = axes[i]\n",
    "        plot_df = summary_df[(summary_df['p'] == p_val) & (summary_df['max_evals_per_run'] == max_evals_values[0])]\n",
    "        sns.lineplot(data=plot_df, x='N', y='mean_wall_time', hue='num_parallel_runs', marker='o', ax=ax, palette='viridis')\n",
    "        ax.set_title(f'p={p_val}, max_evals={max_evals_values[0]}')\n",
    "        ax.set_xlabel('Number of Assets (N)')\n",
    "        ax.set_ylabel('Mean Wall-Clock Time (s)')\n",
    "        ax.legend(title='Parallel Processes')\n",
    "    plt.suptitle('Wall-Clock Time vs. N for Different Parallel Processes', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # --- 2. Speedup vs. Number of Parallel Runs (Parallelization Efficiency) ---\n",
    "    print(\"\\n--- Plotting Speedup vs. Number of Parallel Runs ---\")\n",
    "    fig, axes = plt.subplots(1, len(p_values), figsize=(5 * len(p_values), 6), sharey=True)\n",
    "    if len(p_values) == 1: axes = [axes]\n",
    "\n",
    "    for i, p_val in enumerate(p_values):\n",
    "        ax = axes[i]\n",
    "        plot_df = summary_df[(summary_df['p'] == p_val) & (summary_df['max_evals_per_run'] == max_evals_values[0])]\n",
    "\n",
    "        for N_val in N_values:\n",
    "            baseline_row = plot_df[(plot_df['N'] == N_val) & (plot_df['num_parallel_runs'] == 1)]\n",
    "            if not baseline_row.empty and baseline_row['mean_wall_time'].iloc[0] > 0:\n",
    "                baseline_time = baseline_row['mean_wall_time'].iloc[0]\n",
    "                plot_df_N = plot_df[plot_df['N'] == N_val].copy()\n",
    "                plot_df_N['speedup'] = baseline_time / plot_df_N['mean_wall_time']\n",
    "                sns.lineplot(data=plot_df_N, x='num_parallel_runs', y='speedup', label=f'N={N_val}', marker='o', ax=ax)\n",
    "            else:\n",
    "                print(f\"Warning: Baseline for N={N_val}, p={p_val} not found or time is zero. Skipping speedup plot for this N.\")\n",
    "\n",
    "        ax.plot([1, max(num_parallel_runs_values)], [1, max(num_parallel_runs_values)], 'k--', label='Ideal Speedup')\n",
    "        ax.set_title(f'p={p_val}, max_evals={max_evals_values[0]}')\n",
    "        ax.set_xlabel('Number of Parallel Processes')\n",
    "        ax.set_ylabel('Speedup')\n",
    "        ax.legend(title='Problem Size (N)')\n",
    "    plt.suptitle('Speedup vs. Number of Parallel Processes', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # --- 3. Mean Approximation Ratio (AR) vs. N (Solution Quality Scaling) ---\n",
    "    print(\"\\n--- Plotting Mean Approximation Ratio (AR) vs. N ---\")\n",
    "    fig, axes = plt.subplots(1, len(p_values), figsize=(5 * len(p_values), 6), sharey=False)\n",
    "    if len(p_values) == 1: axes = [axes]\n",
    "\n",
    "    for i, p_val in enumerate(p_values):\n",
    "        ax = axes[i]\n",
    "        plot_df = summary_df[(summary_df['p'] == p_val) & (summary_df['max_evals_per_run'] == max_evals_values[0])]\n",
    "        sns.lineplot(data=plot_df, x='N', y='mean_ar', hue='num_parallel_runs', marker='o', ax=ax, palette='viridis')\n",
    "        ax.set_title(f'p={p_val}, max_evals={max_evals_values[0]}')\n",
    "        ax.set_xlabel('Number of Assets (N)')\n",
    "        ax.set_ylabel('Mean Approximation Ratio (AR)')\n",
    "        ax.legend(title='Parallel Processes')\n",
    "    plt.suptitle('Mean Approximation Ratio vs. N', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # --- 4. Mean Energy vs. max_evals (Optimization Effort vs. Quality) ---\n",
    "    print(\"\\n--- Plotting Mean Energy vs. max_evals ---\")\n",
    "    fixed_N_for_plot = N_values[0] if N_values else None\n",
    "    fixed_p_for_plot = p_values[0] if p_values else None\n",
    "    fixed_procs_for_plot = num_parallel_runs_values[-1] if num_parallel_runs_values else None\n",
    "\n",
    "    if fixed_N_for_plot is not None and fixed_p_for_plot is not None and fixed_procs_for_plot is not None:\n",
    "        plot_df = summary_df[\n",
    "            (summary_df['N'] == fixed_N_for_plot) &\n",
    "            (summary_df['p'] == fixed_p_for_plot) &\n",
    "            (summary_df['num_parallel_runs'] == fixed_procs_for_plot)\n",
    "        ]\n",
    "        if not plot_df.empty:\n",
    "            sns.lineplot(data=plot_df, x='max_evals_per_run', y='mean_energy', marker='o', color='red')\n",
    "            plt.title(f'Mean Best Energy vs. max_evals (N={fixed_N_for_plot}, p={fixed_p_for_plot}, Procs={fixed_procs_for_plot})')\n",
    "            plt.xlabel('Max Function Evaluations per Run')\n",
    "            plt.ylabel('Mean Best Energy')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No data available for plotting Mean Energy vs. max_evals with selected fixed parameters.\")\n",
    "    else:\n",
    "        print(\"Not enough data to plot Mean Energy vs. max_evals.\")\n",
    "\n",
    "    # --- 5. Constraint Satisfaction Rate vs. N (DWE Effectiveness) ---\n",
    "    print(\"\\n--- Plotting Constraint Satisfaction Rate vs. N ---\")\n",
    "    fig, axes = plt.subplots(1, len(p_values), figsize=(5 * len(p_values), 6), sharey=True)\n",
    "    if len(p_values) == 1: axes = [axes]\n",
    "\n",
    "    for i, p_val in enumerate(p_values):\n",
    "        ax = axes[i]\n",
    "        plot_df = summary_df[(summary_df['p'] == p_val) & (summary_df['max_evals_per_run'] == max_evals_values[0])]\n",
    "        sns.lineplot(data=plot_df, x='N', y='constraint_satisfaction_rate', hue='num_parallel_runs', marker='o', ax=ax, palette='viridis')\n",
    "        ax.set_title(f'p={p_val}, max_evals={max_evals_values[0]}')\n",
    "        ax.set_xlabel('Number of Assets (N)')\n",
    "        ax.set_ylabel('Constraint Satisfaction Rate')\n",
    "        ax.set_ylim(-0.05, 1.05) # Ensure y-axis covers 0-1\n",
    "        ax.legend(title='Parallel Processes')\n",
    "    plt.suptitle('Constraint Satisfaction Rate vs. N', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame is empty. Please run Section 4 to generate results before analysis.\")"
   ],
   "id": "e749cea872e83b86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 6. Optional: Analyze the Single Overall Best Result\n",
    "\n",
    "After all benchmarking runs are complete, this section helps you retrieve and analyze the *single best* optimization result (lowest energy) found across *all* the experiments. It provides detailed information about this best-found portfolio and its classical energy.\n",
    "\n",
    "This helps highlight the most successful outcome from your exhaustive search, providing a concrete example for your quantum technology awareness mission."
   ],
   "id": "80c33f657df25104"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "22df09c5a3f90875",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not df_results.empty:\n",
    "    # Sort by best energy to find the overall best result from all runs\n",
    "    overall_best_row = df_results.sort_values(by='best_energy').iloc[0]\n",
    "\n",
    "    print(\"\\n\" + \"---\" * 20)\n",
    "    print(\"### Analyzing the Single Best Result from All Benchmarking Runs ###\")\n",
    "\n",
    "    # Extract details of the best run\n",
    "    best_N = int(overall_best_row['N'])\n",
    "    best_K = int(overall_best_row['K'])\n",
    "    best_p = int(overall_best_row['p'])\n",
    "    best_max_evals = int(overall_best_row['max_evals_per_run'])\n",
    "    best_num_procs = int(overall_best_row['num_parallel_runs'])\n",
    "    best_energy = overall_best_row['best_energy']\n",
    "    best_ar = overall_best_row['best_ar']\n",
    "    most_probable_bitstring = overall_best_row['most_probable_bitstring']\n",
    "    selected_assets_count = int(overall_best_row['selected_assets_count'])\n",
    "    constraint_satisfied = overall_best_row['constraint_satisfied']\n",
    "\n",
    "    print(f\"Overall Best Result Details:\")\n",
    "    print(f\"  Problem Size (N): {best_N}, Assets to Select (K): {best_K}\")\n",
    "    print(f\"  QAOA Depth (p): {best_p}, Max Evals per Run: {best_max_evals}\")\n",
    "    print(f\"  Parallel Processes: {best_num_procs}, Repetition: {int(overall_best_row['repetition'])}\")\n",
    "    print(f\"  Total Wall-Clock Time for this run: {overall_best_row['total_wall_time_s']:.2f} seconds\")\n",
    "\n",
    "    print(f\"\\nOverall Best Energy Found: {best_energy:.8f}\")\n",
    "    print(f\"Corresponding Approximation Ratio: {best_ar}\")\n",
    "    print(f\"Most Probable Bitstring: {most_probable_bitstring}\")\n",
    "    print(f\"Number of Selected Assets: {selected_assets_count} (Expected K={best_K})\")\n",
    "    print(f\"Constraint Satisfied: {'Yes' if constraint_satisfied else 'No'}\")\n",
    "\n",
    "    # To evaluate its classical energy, we need to regenerate the exact problem instance\n",
    "    # using its N, K, and the corresponding problem_seed from the benchmarking loop.\n",
    "    # The problem_seed was generated based on N + p_val + max_evals_per_run + num_procs + rep_idx\n",
    "    reconstructed_problem_seed = int(best_N + best_p + best_max_evals + best_num_procs + overall_best_row['repetition'])\n",
    "    \n",
    "    reconstructed_po_problem, _ = define_po_problem(\n",
    "        best_N, best_K, GLOBAL_Q, GLOBAL_LAMBDA_SUM, \n",
    "        problem_seed=reconstructed_problem_seed\n",
    "    )\n",
    "\n",
    "    # Function to evaluate classical objective energy for a given bitstring\n",
    "    def evaluate_bitstring_energy(bitstring_str, original_mu, original_Sigma, original_q):\n",
    "        x = np.array([int(b) for b in bitstring_str])\n",
    "        portfolio_variance = np.dot(x, np.dot(original_Sigma, x))\n",
    "        portfolio_return = np.dot(original_mu, x)\n",
    "        return original_q * portfolio_variance - portfolio_return\n",
    "\n",
    "    classical_energy_for_best_bitstring = evaluate_bitstring_energy(\n",
    "        most_probable_bitstring,\n",
    "        reconstructed_po_problem['means'], \n",
    "        reconstructed_po_problem['cov'],   \n",
    "        reconstructed_po_problem['q_orig'] \n",
    "    )\n",
    "    print(f\"\\nClassical objective energy for the most probable bitstring: {classical_energy_for_best_bitstring:.6f}\")\n",
    "    print(\"---\" * 20)\n",
    "else:\n",
    "    print(\"No results available for overall best bitstring analysis. Please run Section 4 first.\")"
   ],
   "id": "98df281b2ec36012",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fed65c86e4c2af06",
   "outputs": [],
   "execution_count": null
  }
 ],
 "outputs": [],
 "execution_count": null
}
